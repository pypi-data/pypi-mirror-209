model: &model
  base_config:
    vocab_size: &vocab_size 10000
    embed_size: &embed_size 80
    hidden_size: &hidden_size 256
    dropout_rate: &dropout_rate 0.1
    positional_dropout_rate: &positional_dropout_rate 0.1
    attention_dropout_rate: 0.0
    ctc_weight: 0.3

  embed: &embedding
    module: "mountaintop.layers.speech_embed:SpeechEmbed"
    in_dim: *embed_size
    out_dim: *hidden_size
    pos_type: "rel_pos"
    subsampling_type: "conv2d4" 
    dropout_rate: *dropout_rate
    positional_dropout_rate: *positional_dropout_rate 

  encoder: &encoder
    module: "mountaintop.layers.transformer.encoder:TransformerEncoder"
    block: "conformer"
    dim: *hidden_size
    num_heads: 4
    num_hidden: 2048
    num_blocks: 12
    dropout_rate: *dropout_rate
    attention_dropout_rate: 0.0
    norm_type: "prenorm"
    activation_name: "relu"
    concat_after: false
    # chunk_size: 0
    # use_dynamic_chunk: false
    ### params for conformer
    use_macaron: true
    use_cnn: true
    cnn_kernels: 15
    cnn_norm: "batchnorm" 
    # cnn_norm: "layernorm" 
    cnn_causal: false
    cnn_activation_name: "swish"

  ctc: &ctc
    module: "mountaintop.layers.loss:CTC"
    in_dim: *hidden_size
    num_classes: *vocab_size
    dropout_rate: *dropout_rate
    reduction: "mean"

  decoder: &decoder
    module: "mountaintop.layers.transformer.decoder:TransformerDecoder"
    # module: "mountaintop.layers.transformer.decoder:BiTransformerDecoder"
    block_module: "mountaintop.layers.transformer.decoder_block:TransformerDecoderBlock"
    vocab_size: *vocab_size
    in_dim: *hidden_size
    num_heads: 4
    num_hidden: 2048
    num_blocks: 6
    dropout_rate: *dropout_rate
    positional_dropout_rate: *positional_dropout_rate
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0
    norm_type: "prenorm"
    concat_after: false
    use_output_layer: true

  loss: &loss
    module: "mountaintop.layers.loss:LabelSmoothingKlLoss"
    smoothing: 0.1
    reduction: "mean"
