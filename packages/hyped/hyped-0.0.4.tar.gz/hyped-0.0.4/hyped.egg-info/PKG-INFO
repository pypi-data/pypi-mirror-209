Metadata-Version: 2.1
Name: hyped
Version: 0.0.4
Summary: A collection of data pipelines to ease the training of transformer models
Home-page: https://github.com/ndoll1998/hyped/tree/master
Author: Niclas Doll
Author-email: niclas@amazonis.net
Classifier: License :: Freely Distributable
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3.10
Description-Content-Type: text/markdown
License-File: LICENSE

# :boom: hyped
![Publish Master](https://github.com/ndoll1998/hyped/workflows/PyPI/badge.svg)

A collection of data pipelines to ease the training of transformer models

## installation

The package can be installed from PyPI:

```bash
pip install -U hyped
```

Alternatively you can also install the package from source by cloning this repository

```bash
git clone https://github.com/ndoll1998/hyped.git
cd hyped
pip install -e .
```

## dependencies

`hyped` combines the ðŸ¤— huggingface libraries to implement pipelines that go from data preparation to model training and finally evaluation. The following table shows which library is used as backend to which pipeline stage.

| stage | backend | version |
|:-----:|:-------:|:-------:|
| data preparation | [datasets](https://github.com/huggingface/datasets) | `>=2.12.0` |
| model training | [adapter-transformers](https://github.com/adapter-hub/adapter-transformers) | `>=3.2.1` |
| model evaluation | [evaluate](https://github.com/huggingface/evaluate) | `>=0.4.0` |

## usage

The purpose of `hyped` is to train complex transformer models with minimal effort. The typical ML pipeline consists of three main stages, namely a `data preparation`, a `model training` and a `model evaluation` stage. Each one of these stages can be executed by the `hyped` command line interface.

The following demonstates how to train a text classification model on the [`imdb`](https://huggingface.co/datasets/imdb) dataset by going through all three stages of the ML pipeline (from `examples/run.sh`).

```bash
# data preparation stage
# runs the data preparation pipeline specified in the
# configuration file and saves the prepared dataset
# to the specified location
python -m hyped.scripts.prepare \
    -c examples/text_cls/imdb/distilbert_data.json \
    -o output/distilbert_data

# model training stage
# trains a model on the prepared data generated by the
# previous stage
python -m hyped.scripts.train \
    -c examples/text_cls/imdb/distilbert_run.json \
    -d output/distilbert_data \
    -o output/model

# model evaluation stage
# evaluates the trained model on the test split of the
# prepared dataset generated earlier
python -m hyped.scripts.evaluate \
    -c examples/text_cls/imdb/distilbert_run.json \
    -d output/distilbert_data \
    -m output/model/best-model
```

## features

`hyped` currently implements all components required to train and evaluate models for the following NLP tasks:

 - Text Classification
 - Named Entity Recognition
 - Multi-label Classification


## roadmap

`hyped` is still in its very early stages. With time the goal is to make the framework applicable to a wide variety of tasks and setups. Planned features include the following:

 - ~~support adapter training~~
 - support more tasks
   - Masked Language Modeling
   - Causal Language Modeling
   - nested Named Entity Recognition
   - Question Answering
 - support multi-modal encoders
   - LayoutLM
   - LiLT
 - support distributed training/inference
   - deepspeed
   - pytorch DDP and FSDP
