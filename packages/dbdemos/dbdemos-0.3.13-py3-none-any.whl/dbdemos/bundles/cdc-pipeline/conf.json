{"name": "cdc-pipeline", "category": "data-engineering", "title": "CDC Pipeline with Delta", "description": "Process CDC data to build an entire pipeline and materialize your operational tables in your lakehouse.", "fullDescription": "This demo highlight how to implement a CDC flow (Change Data Capture) with Spark API and Delta Lake.<br/> CDC is typically done ingesting changes from external system (ERP, SQL databases) with tools like fivetran, debezium etc. <br/> In this demo, we'll show you how to re-create your table consuming CDC information. <br/><br/>Ultimately, we'll show you how to programatically scan multiple incoming folder and trigger N stream (1 for each CDC table).<br/>Note that CDC is made easier with Delta Live Table (CDC). We recommend you to try the DLT CDC demo!", "bundle": true, "tags": [{"delta": "Delta Lake"}], "notebooks": [{"path": "_resources/00-setup", "pre_run": false, "publish_on_website": false, "add_cluster_setup_cell": false, "title": "Setup", "description": "Setup."}, {"path": "_resources/01-load-data", "pre_run": false, "publish_on_website": false, "add_cluster_setup_cell": false, "title": "Data initialization", "description": "Data initialization"}, {"path": "01-CDC-CDF-simple-pipeline", "pre_run": true, "publish_on_website": true, "add_cluster_setup_cell": true, "title": "Implement CDC flow with Delta Lake", "description": "Ingest CDC data and materialize your tables and propagate changes downstream."}, {"path": "02-CDC-CDF-full-multi-tables", "pre_run": true, "publish_on_website": true, "add_cluster_setup_cell": true, "title": "Delta Lake Performance & operation", "description": "Programatically ingest multiple CDC flows to synch all your database."}, {"path": "_resources/00-global-setup", "title": "Global init", "description": "Global init", "pre_run": false, "publish_on_website": false, "add_cluster_setup_cell": false, "parameters": {}, "depends_on_previous": true, "libraries": []}]}