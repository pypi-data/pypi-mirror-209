"""A collection of Spark utils."""
# import python libraries
import os
import logging
from typing import List, Optional, Union
from functools import wraps, reduce

from de_utils._utils import to_list

# import pyspark libraries
from pyspark.storagelevel import StorageLevel
from pyspark.sql import (
    SparkSession,
    functions as F,
    Column,
    DataFrame,
    Window
)


from de_utils._logging_utils import get_logger_name


logger = logging.getLogger(get_logger_name(name=__name__))


def assert_active_ss(func):
    """Decorate a function to check for active spark session before execution."""
    @wraps(func)
    def function_wrapper(*args, **kwargs):
        if SparkSession._instantiatedSession is None:
            raise AttributeError(
                "No active session found: "
                "this function needs an active spark session.")

        return func(*args, **kwargs)
    return function_wrapper


@assert_active_ss
def get_active_session() -> SparkSession:
    """
    Return the active SparkSession for the current thread, returned by the builder.

    This allows the Spark session to be used within function/ modules without having
    had the original Spark session object passed to it.

    Returns
    -------
    :class:`SparkSession`
    Spark session if an active session exists otherwise raises error if it doesn't.

    Examples
    --------
    >>> spark = get_active_session()
    >>> spark.sql(....)
    >>> df = spark.createDataFrame(...)
    """
    return SparkSession._instantiatedSession


def get_ss() -> SparkSession:
    """
    Alias for getActiveSession().

    Returns
    -------
    :class:`SparkSession`
    Spark session if an active session exists otherwise raises error if it doesn't.

    Examples
    --------
    # Retrieve current spark session
    >>> spark = get_ss()
    """
    return get_active_session()


def get_spark_ui(hyperlink_text: str = "Spark UI for this session"):
    """
    Display correct hyperlinks for both live and history Spark UI.

    Notes
    ----
    - The "archived" link is available sometime after the session is closed.
      Once the Spark application dies the Spark UI for that session is archived in
      the Yarn history server.
    - To make the UI accessible including when the CDSW/ Spark session has closed or
      timed out add this function call where every Spark session is created.
    """
    from IPython.core.display import display, HTML

    sc = get_ss().sparkContext

    hist = "%s/history/%s" % (sc.getConf().get('spark.yarn.historyServer.address'),
                              sc.applicationId)
    live = "spark-%s.%s" % (os.environ["CDSW_ENGINE_ID"], os.environ["CDSW_DOMAIN"])

    display(HTML(f"<a href='http://{live}'>{hyperlink_text}</a>"
                 f"  (<a href='{hist}'>archived</a>)"))


def eagerEval(max_rows: int = 20, truncate: Union[bool, int] = 20):
    """Enable eager evaluation of dataframes within a spark session.

    When a dataframe is called a HTML formatted table (generated by _repr_html_ df method)
    will be shown with the top 20 or optionally more rows of the data. This is useful when
    viewing data with many columns as it'll offer horizontal scrolling instead of fitting
    the table to the size of the console.


    Note
    ____
    These settings are better defined when the spark session is created, however this
    should provide an easy way to change these settings later on.

    The usage is similar to pandas, returning a dataframe without assignment will display
    a HTML table of the data e.g. df == df.show() and df.limit(5) == df.show(5).

    Note that df.limit(x) will only display the max_rows defined, so to show more than 20
    rows change max_rows setting beforehand.
    """
    spark = get_ss()

    for conf, arg in [
        ('enabled', 'true'),
        ('truncate', '999' if truncate is False else truncate),
        ('maxNumRows', max_rows)
    ]:
        spark.conf.set(f'spark.sql.repl.eagerEval.{conf}', arg)


def get_hive_context():
    """Get existig HIVE/SQL context.

    This provides the refreshtable function in addition to those in the SQL context.
    """
    from pyspark import HiveContext

    return HiveContext(get_active_session())


def named_cache(
    df: DataFrame,
    name: str,
    eager: Optional[bool] = False
):
    """Persist df to MEMORY_AND_DISK with a named catalog reference via tempview.

    Parameters
    ----------
    df : DataFrame

    name : str
        The name the df will have in the catalog and will show under in the Spark UI.

    eager: bool
        Whether to perform the cache lazy (default) or immediately.  This is an
        alternative to using df.count() to force the df to cache immediately.
    """
    # easier to get sql context from df
    sc = df._jdf.sqlContext()

    df.createOrReplaceTempView(name)

    if eager:
        # SQL cache default is eager
        sc.sql(f'CACHE TABLE {name}')
    else:
        sc.sql(f'CACHE TABLE LAZY {name}')


def named_persist(
    df: DataFrame,
    name: str,
    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK
):
    """
    Persist df to a storage level giving it a reference in the catalog via a tempview.

    Default storage level is MEMORY_AND_DISK inline with PySpark persist() function.
    Please refer to the PySpark Persist() function docs as the use is identical other
    than the name parameter.

    Parameters
    ----------
    df : DataFrame
    name : str
        The name the df will have in the catalog and will show under in the Spark UI.
    storageLevel: pyspark.sql.storageLevel
        Define the storage level for the df.

    Examples
    --------
    >>> from pyspark.storagelevel import StorageLevel
    >>> named_persist(df, name='example', storageLevel=StorageLevel.DISK_ONLY, eager)
    """
    # saves getting active spark session to access the java catalog
    jcatalog = get_active_session()._jsparkSession.catalog()

    df.createTempView(name)
    jcatalog.cacheTable(name, df._sc._getJavaStorageLevel(storageLevel))


def named_unpersist(name: str, drop_table: bool = True):
    """Unpersist cached df and drop temp view or optionally keeping the temp view."""
    catalog = get_active_session().catalog
    catalog.uncacheTable(name)

    if drop_table:
        catalog.dropTempView(name)


def align_date_to_a_specific_weekday(
    dates: str,
    week_start: str = 'Sun',
    align_day: str = 'Sun'
) -> Column:
    """
    Align dates to a certain day of the week in the current calendar week.

    Dates that fall on the specified day (e.g. Sundays when align_day=='Sun')
    are not shifted.

    Parameters
    ----------
    dates : str
        Column name containing the dates.
    week_start: str {Sun, Mon, Tue, Wed, Thu, Fri, Sat}, default Sun
        Day the week starts on.
    align_day : str {Sun, Mon, Tue, Wed, Thu, Fri, Sat}, default Sun
        The day of the week to align daily data to.
    Returns
    -------
    Column
        Column with the dates aligned to the specified day of the
        current calendar week, as defined by the chosen week start day.
    """
    days = {'Sun': 0, 'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6}

    # exception handling
    if week_start not in days or align_day not in days:
        raise ValueError(
            'Values for week_start and align_day must be in'
            ' {Sun, Mon, Tue, Wed, Thu, Fri, Sat} but received'
           f' {week_start} and {align_day} respectively'
        )

    offset = days[week_start]

    # add 1 because pyspark dayofweek counts Sunday as 1 and Saturday as 7
    target = ((days[align_day] - offset) % 7) + 1

    # need to subtract 1 to return to 0-6 for modulus to work
    # also for some reason need numbers positive for modulus, so add 7
    # then convert back to 1-7 by adding 1
    offset_day = f'(((((dayofweek({dates}) - {offset}) - 1) + 7) % 7) + 1)'

    return F.expr(f'''
            CASE WHEN {offset_day} <= {target}
            THEN date_add({dates}, {target} - {offset_day})
            ELSE date_sub({dates}, {offset_day} - {target})
            END
            ''')


def any_in(condition: Column, window_spec: Window) -> Column:
    """Check if a condition is true at least once within a group defined by the window.

    Parameters
    ----------
    condition : Column
    window_spec : Window

    Returns
    -------
    Spark column of boolean type
        Returns a boolean mask over each group/ partition with true if the
        condition is true at least once and when not false.

    Examples
    --------
    >>> # remove entire group if one row meets the condition
    >>>from pyspark.sql import Window
    >>>
    >>> window_spec = Window.partitionBy('Name1')
    >>> condition = any_in(F.col('Name2').isNull(), window_spec))
    >>> df = df.withColumn('check_group', condition)
    >>> +-----+-----+---+------------+
    >>> |Name1|Name2|key|        flag|
    >>> +-----+-----+---+------------+
    >>> |    B|    X|  1|       false|
    >>> |    B|    W|  1|       false|
    >>> |    C|    V|  1|       false|
    >>> |    A|    Z|  1|        true|
    >>> |    A| null|  1|        true|
    >>> +-----+-----+---+------------+
    >>>
    >>> df.filter('check_group').drop('check_group')
    >>> +-----+-----+---+
    >>> |Name1|Name2|key|
    >>> +-----+-----+---+
    >>> |    B|    X|  1|
    >>> |    B|    W|  1|
    >>> |    C|    V|  1|
    >>> +-----+-----+---+
    >>>
    >>> # or apply a function to a group with when()
    >>> df.withColumn(..., when(condition, do_this_to_group).otherwise(...))
    """
    # propagate max/ true value across the window if there is at least one otherwise false
    return F.max(condition).over(spark_window_spec)


def _add_missing_cols(df, cols, fill):
    """Add null filled cols from provided list if they don't already in the given df."""
    return df[['*', *[F.lit(fill).name(c) for c in cols if c not in df.columns]]]


def union_by_name(
    dfs: List[DataFrame],
    allow_missing_cols: bool = False,
    fill: Union[float, str, None] = None
) -> DataFrame:
    """Union a list of dataframes by name, optionally keeping non-overlapping columns.

    Parameters
    ------
    dfs: List of dataframes

    allow_missing_cols: bool
        If True, columns are added to all dataframes where they are missing. If False
        it reverts to the default behaviour of DataFrame.unionByName.

    fill: any string or numeric value
        If non-overlaping/missing columns are kept they're padded with this parameter,
        by default this is null.

    Returns
    -------
    DataFrame
    """
    if allow_missing_cols:
        # get unique cols from across all dataframes keeping the order
        all_cols = list(
            dict.fromkeys(
                (c for cols in (c.columns for c in dfs) for c in cols)
            )
        )
        # standardise columns
        dfs = (_add_missing_cols(df, all_cols, fill) for df in dfs)

    return reduce(DataFrame.unionByName, dfs)


def cast_columns(df, col_type_dict, validate_cols=True):
    """
    Cast specified columns to specific types.

    Parameter:
    ----------
    df: DataFrame
    col_type_dict: dict
        The dictionary format should be {'float': ['col1', 'col2'], 'string': [...]}
        and use the simple string name of the Spark data type e.g.
        IntergerType =='int'
        DoubleType == 'double'
        FloatType == 'float'
        BooleanType == 'boolean'
        DateType == 'date'
        TimestampType == 'timestamp'
        StringType == 'string'
        DecimalType == 'decimal'
    validate_cols: bool
        Whether to raise an error if a col name in col_type_dict doesn't exist in df.

    Returns:
    -------
    DataFrame: df with renamed columns.
    """
    # Flatten list values and invert mapping.
    d = {v: k for k, vs in col_type_dict.items() for v in vs}

    if validate_cols:
        cols = [c for c in d if c not in df.columns]
        if cols:
            raise ValueError(
                f"{', '.join(cols)} cols aren't valid column names, try: {df.columns}")

    return df[[F.col(c).cast(d[c]) if c in d else c for c in df.columns]]


def rm_spaces(col: str) -> Column:
    """
    Remove any whitespace from a string.

    Parameters
    ----------
    col : str
        column name in the table

    Returns
    -------
    Column object with whitespaces removed
    """
    return F.regexp_replace(F.col(col), " ", "")


def melt(
    df: DataFrame,
    id_vars: Union[str, List[str]],
    value_vars: Union[str, List[str]],
    var_name: str = "variable",
    value_name: str = "value"
) -> DataFrame:
    """Melt a spark dataframe in a pandas like fashion.

    Parameters
    ----------
    df : DataFrame
        The pyspark dataframe to melt.
    id_vars : list(str), str
        Name of columns to use as identifier variables.
    value_vars : list(str), str
        Name of columns to unpivot.
    var_name : str
        Name of the target column containing variable names
        (i.e., the original column names).
    value_name : str
        Name of column of with the values for ‘value’ column.

    Returns
    -------
    DataFrame
        The "melted" input data as a pyspark data frame.

    Examples
    --------
    >>> df = spark.createDataFrame(
    ...     [[1, 2, 3, 4],
    ...      [5, 6, 7, 8],
    ...      [9, 10, 11, 12]],
    ...     ["col1", "col2", "col3", "col4"])
    >>> melt(df=df, id_vars="col1", value_vars=["col2", "col3"]).show()
    +----+--------+-----+
    |col1|variable|value|
    +----+--------+-----+
    |   1|    col2|    2|
    |   1|    col3|    3|
    |   5|    col2|    6|
    |   5|    col3|    7|
    |   9|    col2|   10|
    |   9|    col3|   11|
    +----+--------+-----+

    >>> melt(df=df, id_vars=["col1", "col2"], value_vars=["col3", "col4"]
    ... ).show()
    +----+----+--------+-----+
    |col1|col2|variable|value|
    +----+----+--------+-----+
    |   1|   2|    col3|    3|
    |   1|   2|    col4|    4|
    |   5|   6|    col3|    7|
    |   5|   6|    col4|    8|
    |   9|  10|    col3|   11|
    |   9|  10|    col4|   12|
    +----+----+--------+-----+
    """
    id_vars = to_list(id_vars)
    value_vars = to_list(value_vars)

    # Create array<struct<variable: str, value: ...>, <struct<...>>
    # Essentially a list of column placeholders
    # Tuple comprehension ensures we have a lit element
    # and col reference for each column to melt
    _vars_and_vals = F.array(*(
        F.struct(F.lit(c).alias(var_name), F.col(c).alias(value_name))
        for c in value_vars))

    # Add to the DataFrame and explode, which extends the dataframe
    _tmp = df.withColumn("_vars_and_vals", F.explode(_vars_and_vals))

    # We only want to select certain columns
    cols = id_vars + [
        F.col("_vars_and_vals")[x].alias(x) for x in [var_name, value_name]]

    return _tmp.select(*cols)
